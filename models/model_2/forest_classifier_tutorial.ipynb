{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå≤ Forest Pattern Recognition Tutorial\n",
    "\n",
    "**Random Forest Classifier for Deforestation Risk Assessment**\n",
    "\n",
    "This notebook walks through building a machine learning system to classify forest areas by risk level based on:\n",
    "- NDVI (vegetation health)\n",
    "- Texture features (canopy structure)\n",
    "- Spatial patterns (fragmentation)\n",
    "\n",
    "## Outline:\n",
    "1. Setup and Data Generation\n",
    "2. Feature Engineering\n",
    "3. Model Training\n",
    "4. Evaluation\n",
    "5. Risk Map Generation\n",
    "6. Real-World Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install scikit-learn numpy pandas matplotlib seaborn scipy scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"‚úì Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding NDVI üåø\n",
    "\n",
    "**NDVI (Normalized Difference Vegetation Index)** measures vegetation health:\n",
    "\n",
    "$$NDVI = \\frac{NIR - Red}{NIR + Red}$$\n",
    "\n",
    "- **NIR**: Near-Infrared reflectance\n",
    "- **Red**: Red band reflectance\n",
    "\n",
    "### Interpretation:\n",
    "- **0.6 to 1.0**: Dense, healthy vegetation üå≤\n",
    "- **0.2 to 0.6**: Sparse vegetation üåæ\n",
    "- **-1.0 to 0.2**: No vegetation (water, bare soil, urban) üèúÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndvi(nir, red):\n",
    "    \"\"\"Calculate NDVI from NIR and Red bands\"\"\"\n",
    "    return (nir - red) / (nir + red + 1e-10)\n",
    "\n",
    "# Example calculation\n",
    "nir_healthy = 0.5\n",
    "red_healthy = 0.1\n",
    "ndvi_healthy = calculate_ndvi(nir_healthy, red_healthy)\n",
    "\n",
    "nir_degraded = 0.2\n",
    "red_degraded = 0.3\n",
    "ndvi_degraded = calculate_ndvi(nir_degraded, red_degraded)\n",
    "\n",
    "print(f\"Healthy Forest NDVI: {ndvi_healthy:.3f}\")\n",
    "print(f\"Degraded Area NDVI: {ndvi_degraded:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Data üé≤\n",
    "\n",
    "We'll create synthetic forest patches with different risk levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forest_patch(risk_level, size=32):\n",
    "    \"\"\"\n",
    "    Generate a synthetic NDVI patch\n",
    "    \n",
    "    Args:\n",
    "        risk_level: 'Low', 'Medium', or 'High'\n",
    "        size: patch dimension\n",
    "    \"\"\"\n",
    "    if risk_level == 'Low':\n",
    "        # Healthy forest: high NDVI, low variability\n",
    "        base = np.random.uniform(0.65, 0.85)\n",
    "        noise = np.random.normal(0, 0.05, (size, size))\n",
    "        \n",
    "    elif risk_level == 'Medium':\n",
    "        # Some disturbance: medium NDVI, moderate variability\n",
    "        base = np.random.uniform(0.35, 0.55)\n",
    "        noise = np.random.normal(0, 0.1, (size, size))\n",
    "        \n",
    "        # Add degradation patches\n",
    "        patch = np.clip(base + noise, 0, 1)\n",
    "        for _ in range(np.random.randint(1, 3)):\n",
    "            x, y = np.random.randint(0, size-10, 2)\n",
    "            w, h = np.random.randint(5, 12, 2)\n",
    "            patch[y:y+h, x:x+w] *= np.random.uniform(0.4, 0.7)\n",
    "        return patch\n",
    "        \n",
    "    else:  # High risk\n",
    "        # Severe degradation: low NDVI, high variability\n",
    "        base = np.random.uniform(0.1, 0.35)\n",
    "        noise = np.random.normal(0, 0.15, (size, size))\n",
    "        \n",
    "        # Add large degraded areas\n",
    "        patch = np.clip(base + noise, 0, 1)\n",
    "        for _ in range(np.random.randint(2, 5)):\n",
    "            x, y = np.random.randint(0, size-15, 2)\n",
    "            w, h = np.random.randint(8, 20, 2)\n",
    "            patch[y:y+h, x:x+w] *= np.random.uniform(0.1, 0.4)\n",
    "        return patch\n",
    "    \n",
    "    return np.clip(base + noise, 0, 1)\n",
    "\n",
    "# Generate samples\n",
    "n_samples_per_class = 100\n",
    "patch_size = 32\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for risk in ['Low', 'Medium', 'High']:\n",
    "    for _ in range(n_samples_per_class):\n",
    "        patch = generate_forest_patch(risk, patch_size)\n",
    "        images.append(patch)\n",
    "        labels.append(risk)\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"‚úì Generated {len(images)} forest patches\")\n",
    "print(f\"  Shape: {images.shape}\")\n",
    "print(f\"  Classes: {np.unique(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "\n",
    "for i, risk in enumerate(['Low', 'Medium', 'High']):\n",
    "    risk_indices = np.where(labels == risk)[0]\n",
    "    \n",
    "    for j in range(4):\n",
    "        idx = risk_indices[j]\n",
    "        patch = images[idx]\n",
    "        \n",
    "        im = axes[i, j].imshow(patch, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "        axes[i, j].set_title(f'{risk} Risk\\nMean NDVI: {patch.mean():.2f}')\n",
    "        axes[i, j].axis('off')\n",
    "        \n",
    "        if j == 3:\n",
    "            plt.colorbar(im, ax=axes[i, j], fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering üîß\n",
    "\n",
    "Extract meaningful features from NDVI patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(ndvi_patch):\n",
    "    \"\"\"\n",
    "    Extract features from an NDVI patch\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    features['mean'] = np.mean(ndvi_patch)\n",
    "    features['std'] = np.std(ndvi_patch)\n",
    "    features['min'] = np.min(ndvi_patch)\n",
    "    features['max'] = np.max(ndvi_patch)\n",
    "    features['median'] = np.median(ndvi_patch)\n",
    "    features['range'] = features['max'] - features['min']\n",
    "    \n",
    "    # Percentiles\n",
    "    features['p25'] = np.percentile(ndvi_patch, 25)\n",
    "    features['p75'] = np.percentile(ndvi_patch, 75)\n",
    "    features['iqr'] = features['p75'] - features['p25']\n",
    "    \n",
    "    # Vegetation categories\n",
    "    dense = (ndvi_patch >= 0.6).sum() / ndvi_patch.size\n",
    "    sparse = ((ndvi_patch >= 0.3) & (ndvi_patch < 0.6)).sum() / ndvi_patch.size\n",
    "    bare = (ndvi_patch < 0.3).sum() / ndvi_patch.size\n",
    "    \n",
    "    features['dense_veg_ratio'] = dense\n",
    "    features['sparse_veg_ratio'] = sparse\n",
    "    features['bare_ratio'] = bare\n",
    "    \n",
    "    # Texture (simple approximation)\n",
    "    features['texture_h'] = np.std(np.diff(ndvi_patch, axis=0))\n",
    "    features['texture_v'] = np.std(np.diff(ndvi_patch, axis=1))\n",
    "    \n",
    "    # Coefficient of variation\n",
    "    features['cv'] = features['std'] / (features['mean'] + 1e-7)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for all samples\n",
    "print(\"Extracting features...\")\n",
    "feature_list = []\n",
    "for patch in images:\n",
    "    feats = extract_features(patch)\n",
    "    feature_list.append(feats)\n",
    "\n",
    "# Convert to DataFrame\n",
    "features_df = pd.DataFrame(feature_list)\n",
    "features_df['label'] = labels\n",
    "\n",
    "print(f\"‚úì Extracted {len(features_df.columns)-1} features\")\n",
    "print(\"\\nFeature names:\")\n",
    "print(list(features_df.columns[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "important_features = ['mean', 'std', 'dense_veg_ratio', 'texture_h', 'cv', 'range']\n",
    "\n",
    "for i, feat in enumerate(important_features):\n",
    "    for risk in ['Low', 'Medium', 'High']:\n",
    "        data = features_df[features_df['label'] == risk][feat]\n",
    "        axes[i].hist(data, alpha=0.5, label=risk, bins=20)\n",
    "    \n",
    "    axes[i].set_xlabel(feat)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how features differ between risk classes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = features_df.drop('label', axis=1).values\n",
    "y = features_df['label'].values\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úì Data prepared and scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest Classifier...\\n\")\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Low', 'Medium', 'High'],\n",
    "            yticklabels=['Low', 'Medium', 'High'])\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_names = features_df.columns[:-1]\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(indices)), importances[indices])\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for i in range(5):\n",
    "    idx = indices[i]\n",
    "    print(f\"{i+1}. {feature_names[idx]}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation üîÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross-Validation\n",
    "print(\"Running 5-fold cross-validation...\\n\")\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    model, \n",
    "    scaler.transform(X),\n",
    "    y,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Cross-Validation Scores:\")\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"  Fold {i}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Risk Map Generation üó∫Ô∏è\n",
    "\n",
    "Apply the model to a large area by dividing it into grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_risk_map(model, scaler, ndvi_map, grid_size=32):\n",
    "    \"\"\"\n",
    "    Generate risk classification map\n",
    "    \"\"\"\n",
    "    h, w = ndvi_map.shape\n",
    "    n_rows = h // grid_size\n",
    "    n_cols = w // grid_size\n",
    "    \n",
    "    risk_map = np.empty((n_rows, n_cols), dtype=object)\n",
    "    prob_map = np.zeros((n_rows, n_cols, 3))  # Low, Medium, High\n",
    "    \n",
    "    print(f\"Processing {n_rows}x{n_cols} = {n_rows*n_cols} grid cells...\")\n",
    "    \n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            # Extract cell\n",
    "            cell = ndvi_map[\n",
    "                i*grid_size:(i+1)*grid_size,\n",
    "                j*grid_size:(j+1)*grid_size\n",
    "            ]\n",
    "            \n",
    "            # Extract features\n",
    "            feats = extract_features(cell)\n",
    "            X_cell = np.array([list(feats.values())])\n",
    "            X_cell_scaled = scaler.transform(X_cell)\n",
    "            \n",
    "            # Predict\n",
    "            risk_map[i, j] = model.predict(X_cell_scaled)[0]\n",
    "            prob_map[i, j] = model.predict_proba(X_cell_scaled)[0]\n",
    "    \n",
    "    print(\"‚úì Risk map generated\")\n",
    "    return risk_map, prob_map\n",
    "\n",
    "# Generate synthetic large area\n",
    "large_size = 512\n",
    "print(\"\\nGenerating synthetic NDVI map...\")\n",
    "\n",
    "ndvi_large = np.zeros((large_size, large_size))\n",
    "\n",
    "# Healthy forest (top-left)\n",
    "ndvi_large[:256, :256] = np.clip(np.random.normal(0.72, 0.08, (256, 256)), 0, 1)\n",
    "\n",
    "# Medium risk (top-right)\n",
    "ndvi_large[:256, 256:] = np.clip(np.random.normal(0.45, 0.12, (256, 256)), 0, 1)\n",
    "\n",
    "# High risk (bottom)\n",
    "ndvi_large[256:, :] = np.clip(np.random.normal(0.25, 0.15, (256, 512)), 0, 1)\n",
    "\n",
    "print(f\"‚úì NDVI map created: {ndvi_large.shape}\")\n",
    "\n",
    "# Generate risk map\n",
    "risk_map, prob_map = create_risk_map(model, scaler, ndvi_large, grid_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Risk Map\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Original NDVI\n",
    "im0 = axes[0, 0].imshow(ndvi_large, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[0, 0].set_title('NDVI Map', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im0, ax=axes[0, 0])\n",
    "\n",
    "# Risk Classification\n",
    "risk_numeric = np.vectorize({'Low': 0, 'Medium': 1, 'High': 2}.get)(risk_map)\n",
    "im1 = axes[0, 1].imshow(risk_numeric, cmap='RdYlGn_r', vmin=0, vmax=2)\n",
    "axes[0, 1].set_title('Risk Classification', fontsize=12, fontweight='bold')\n",
    "cbar = plt.colorbar(im1, ax=axes[0, 1], ticks=[0, 1, 2])\n",
    "cbar.set_ticklabels(['Low', 'Medium', 'High'])\n",
    "\n",
    "# High Risk Probability\n",
    "im2 = axes[1, 0].imshow(prob_map[:, :, 2], cmap='Reds', vmin=0, vmax=1)\n",
    "axes[1, 0].set_title('High Risk Probability', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1, 0])\n",
    "\n",
    "# Statistics\n",
    "axes[1, 1].axis('off')\n",
    "total = risk_map.size\n",
    "low = (risk_map == 'Low').sum()\n",
    "medium = (risk_map == 'Medium').sum()\n",
    "high = (risk_map == 'High').sum()\n",
    "\n",
    "stats_text = f\"\"\"\n",
    "RISK ASSESSMENT SUMMARY\n",
    "{'='*40}\n",
    "\n",
    "Total Cells: {total}\n",
    "\n",
    "Low Risk:    {low:3d} ({low/total*100:5.1f}%)\n",
    "Medium Risk: {medium:3d} ({medium/total*100:5.1f}%)\n",
    "High Risk:   {high:3d} ({high/total*100:5.1f}%)\n",
    "\n",
    "{'='*40}\n",
    "\n",
    "‚ö†Ô∏è  Critical Areas: {high} cells\n",
    "   require immediate attention\n",
    "\n",
    "Mean NDVI: {ndvi_large.mean():.3f}\n",
    "\"\"\"\n",
    "\n",
    "axes[1, 1].text(0.1, 0.5, stats_text, \n",
    "                fontsize=11, \n",
    "                family='monospace',\n",
    "                verticalalignment='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model üíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model and scaler\n",
    "joblib.dump({\n",
    "    'model': model,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': list(feature_names)\n",
    "}, 'forest_risk_classifier.pkl')\n",
    "\n",
    "print(\"‚úì Model saved: forest_risk_classifier.pkl\")\n",
    "print(\"\\nTo load later:\")\n",
    "print(\">>> data = joblib.load('forest_risk_classifier.pkl')\")\n",
    "print(\">>> model = data['model']\")\n",
    "print(\">>> scaler = data['scaler']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Real-World Usage Example üåç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_area(model, scaler, ndvi_patch):\n",
    "    \"\"\"\n",
    "    Predict risk for a single area\n",
    "    \n",
    "    Args:\n",
    "        model: trained classifier\n",
    "        scaler: fitted StandardScaler\n",
    "        ndvi_patch: 2D NDVI array\n",
    "    \n",
    "    Returns:\n",
    "        prediction, probabilities\n",
    "    \"\"\"\n",
    "    # Extract features\n",
    "    feats = extract_features(ndvi_patch)\n",
    "    X = np.array([list(feats.values())])\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(X_scaled)[0]\n",
    "    probabilities = model.predict_proba(X_scaled)[0]\n",
    "    \n",
    "    # Format results\n",
    "    class_names = model.classes_\n",
    "    prob_dict = {cls: prob for cls, prob in zip(class_names, probabilities)}\n",
    "    \n",
    "    return prediction, prob_dict\n",
    "\n",
    "# Example: Test on new data\n",
    "print(\"Testing on new forest patches:\\n\")\n",
    "\n",
    "# Test 1: Healthy forest\n",
    "test_healthy = generate_forest_patch('Low', 32)\n",
    "pred, probs = predict_single_area(model, scaler, test_healthy)\n",
    "print(f\"Test 1 - Healthy Forest:\")\n",
    "print(f\"  Mean NDVI: {test_healthy.mean():.3f}\")\n",
    "print(f\"  Prediction: {pred}\")\n",
    "print(f\"  Confidence: {max(probs.values()):.1%}\")\n",
    "print()\n",
    "\n",
    "# Test 2: Degraded area\n",
    "test_degraded = generate_forest_patch('High', 32)\n",
    "pred, probs = predict_single_area(model, scaler, test_degraded)\n",
    "print(f\"Test 2 - Degraded Area:\")\n",
    "print(f\"  Mean NDVI: {test_degraded.mean():.3f}\")\n",
    "print(f\"  Prediction: {pred}\")\n",
    "print(f\"  Confidence: {max(probs.values()):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. ‚úÖ What NDVI is and how to interpret it\n",
    "2. ‚úÖ How to engineer features from satellite imagery\n",
    "3. ‚úÖ How to train a Random Forest classifier\n",
    "4. ‚úÖ How to evaluate model performance\n",
    "5. ‚úÖ How to generate risk classification maps\n",
    "6. ‚úÖ How to save and deploy the model\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Use Real Data**: Load actual satellite imagery (Sentinel-2, Landsat)\n",
    "- **Add More Features**: Include elevation, slope, distance to roads\n",
    "- **Time Series**: Track changes over multiple dates\n",
    "- **Advanced Models**: Try XGBoost, LightGBM, or deep learning\n",
    "- **Deploy**: Create a web service or GIS plugin\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/)\n",
    "- [Sentinel-2 Data](https://scihub.copernicus.eu/)\n",
    "- [Google Earth Engine](https://earthengine.google.com/)\n",
    "\n",
    "**Happy forest monitoring! üå≤üìä**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
