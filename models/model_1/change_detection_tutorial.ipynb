{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå≤ Change Detection CNN - Interactive Tutorial\n",
    "\n",
    "Learn to build and train a Convolutional Neural Network for detecting deforestation from paired satellite/drone images.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. Model architecture basics\n",
    "2. Data preparation\n",
    "3. Training process\n",
    "4. Evaluation metrics\n",
    "5. Making predictions\n",
    "6. Real-world application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install tensorflow numpy matplotlib opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Change Detection üîç\n",
    "\n",
    "**Goal:** Detect WHERE forest has been removed between two time periods\n",
    "\n",
    "**Input:** Two images\n",
    "- T‚ÇÄ (Before) - Baseline image\n",
    "- T‚ÇÅ (After) - Recent image\n",
    "\n",
    "**Output:** Probability map\n",
    "- Values 0-1 for each pixel\n",
    "- High values = deforestation likely\n",
    "- Low values = no change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Model üèóÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_change_detection_model(img_size=256, n_channels=3):\n",
    "    \"\"\"\n",
    "    Build a CNN for change detection\n",
    "    \n",
    "    Architecture:\n",
    "    1. Two inputs (T0 and T1)\n",
    "    2. Concatenate\n",
    "    3. Encoder (feature extraction)\n",
    "    4. Decoder (mask reconstruction)\n",
    "    5. Output (probability map)\n",
    "    \"\"\"\n",
    "    \n",
    "    # === INPUTS ===\n",
    "    input_t0 = layers.Input(shape=(img_size, img_size, n_channels), name='image_t0')\n",
    "    input_t1 = layers.Input(shape=(img_size, img_size, n_channels), name='image_t1')\n",
    "    \n",
    "    # Concatenate both images\n",
    "    merged = layers.Concatenate(name='merge')([input_t0, input_t1])\n",
    "    \n",
    "    # === ENCODER (Downsampling) ===\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu', name='enc_conv1')(merged)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(2, name='enc_pool1')(x)\n",
    "    \n",
    "    # Block 2\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu', name='enc_conv2')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(2, name='enc_pool2')(x)\n",
    "    \n",
    "    # Block 3\n",
    "    x = layers.Conv2D(256, 3, padding='same', activation='relu', name='enc_conv3')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(2, name='enc_pool3')(x)\n",
    "    \n",
    "    # Bottleneck\n",
    "    x = layers.Conv2D(512, 3, padding='same', activation='relu', name='bottleneck')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # === DECODER (Upsampling) ===\n",
    "    # Upsample 1\n",
    "    x = layers.UpSampling2D(2, name='dec_upsample1')(x)\n",
    "    x = layers.Conv2D(256, 3, padding='same', activation='relu', name='dec_conv1')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Upsample 2\n",
    "    x = layers.UpSampling2D(2, name='dec_upsample2')(x)\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu', name='dec_conv2')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Upsample 3\n",
    "    x = layers.UpSampling2D(2, name='dec_upsample3')(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu', name='dec_conv3')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # === OUTPUT ===\n",
    "    output = layers.Conv2D(1, 1, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(inputs=[input_t0, input_t1], outputs=output, name='ChangeDetectionCNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_change_detection_model(img_size=256)\n",
    "print(f\"\\n‚úì Model created with {model.count_params():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compile Model ‚öôÔ∏è\n",
    "\n",
    "Define loss function and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Dice coefficient for segmentation tasks\"\"\"\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    \"\"\"BCE + Dice Loss\"\"\"\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    return bce + dice_loss(y_true, y_pred)\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=combined_loss,\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        dice_coefficient,\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"‚úì Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Training Data üé≤\n",
    "\n",
    "For this tutorial, we'll generate synthetic data. In production, use real satellite/drone images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=200, img_size=256):\n",
    "    \"\"\"\n",
    "    Generate synthetic change detection data\n",
    "    \"\"\"\n",
    "    print(f\"Generating {n_samples} synthetic samples...\")\n",
    "    \n",
    "    images_t0 = []\n",
    "    images_t1 = []\n",
    "    masks = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create base forest image (T0)\n",
    "        img_t0 = np.random.rand(img_size, img_size, 3).astype(np.float32)\n",
    "        img_t0[:, :, 1] *= 0.8  # More green\n",
    "        \n",
    "        # Create T1 with some changes\n",
    "        img_t1 = img_t0.copy()\n",
    "        \n",
    "        # Create change mask\n",
    "        mask = np.zeros((img_size, img_size, 1), dtype=np.float32)\n",
    "        \n",
    "        # Add random deforestation patches\n",
    "        n_patches = np.random.randint(1, 5)\n",
    "        for _ in range(n_patches):\n",
    "            x = np.random.randint(0, img_size - 50)\n",
    "            y = np.random.randint(0, img_size - 50)\n",
    "            w = np.random.randint(20, 50)\n",
    "            h = np.random.randint(20, 50)\n",
    "            \n",
    "            # Change image T1\n",
    "            img_t1[y:y+h, x:x+w, :] *= 0.5\n",
    "            img_t1[y:y+h, x:x+w, 2] += 0.2  # More brown\n",
    "            \n",
    "            # Mark in mask\n",
    "            mask[y:y+h, x:x+w, 0] = 1.0\n",
    "        \n",
    "        images_t0.append(img_t0)\n",
    "        images_t1.append(img_t1)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    print(f\"‚úì Generated {n_samples} samples\")\n",
    "    return np.array(images_t0), np.array(images_t1), np.array(masks)\n",
    "\n",
    "# Generate data\n",
    "X_t0, X_t1, y = generate_synthetic_data(n_samples=200, img_size=256)\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  X_t0: {X_t0.shape}\")\n",
    "print(f\"  X_t1: {X_t1.shape}\")\n",
    "print(f\"  y: {y.shape}\")\n",
    "print(f\"\\nChange percentage: {y.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(0, len(X_t0))\n",
    "    \n",
    "    axes[i, 0].imshow(X_t0[idx])\n",
    "    axes[i, 0].set_title(f'Sample {idx} - T‚ÇÄ (Before)')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(X_t1[idx])\n",
    "    axes[i, 1].set_title(f'Sample {idx} - T‚ÇÅ (After)')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    axes[i, 2].imshow(y[idx, :, :, 0], cmap='Reds', vmin=0, vmax=1)\n",
    "    axes[i, 2].set_title(f'Sample {idx} - Change Mask')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Val Split üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (80/20)\n",
    "split_idx = int(0.8 * len(X_t0))\n",
    "\n",
    "X_t0_train, X_t0_val = X_t0[:split_idx], X_t0[split_idx:]\n",
    "X_t1_train, X_t1_val = X_t1[:split_idx], X_t1[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(X_t0_train)}\")\n",
    "print(f\"Validation samples: {len(X_t0_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure callbacks\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        f'best_model_{timestamp}.keras',\n",
    "        monitor='val_dice_coefficient',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"  ‚úì ModelCheckpoint\")\n",
    "print(\"  ‚úì EarlyStopping (patience=10)\")\n",
    "print(\"  ‚úì ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_t0_train, X_t1_train],\n",
    "    y_train,\n",
    "    validation_data=([X_t0_val, X_t1_val], y_val),\n",
    "    epochs=30,\n",
    "    batch_size=8,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì TRAINING COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Results üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0, 0].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0, 1].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Dice\n",
    "axes[1, 0].plot(history.history['dice_coefficient'], label='Train', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_dice_coefficient'], label='Validation', linewidth=2)\n",
    "axes[1, 0].set_title('Dice Coefficient', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Dice')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Precision & Recall\n",
    "axes[1, 1].plot(history.history['precision'], label='Precision (Train)', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_precision'], label='Precision (Val)', linewidth=2)\n",
    "axes[1, 1].plot(history.history['recall'], label='Recall (Train)', linewidth=2, linestyle='--')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Recall (Val)', linewidth=2, linestyle='--')\n",
    "axes[1, 1].set_title('Precision & Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'training_results_{timestamp}.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Plot saved: training_results_{timestamp}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"\\nüìä FINAL EVALUATION\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = model.evaluate([X_t0_val, X_t1_val], y_val, verbose=0)\n",
    "\n",
    "metrics = ['Loss', 'Accuracy', 'Dice Coefficient', 'Precision', 'Recall']\n",
    "for name, value in zip(metrics, results):\n",
    "    print(f\"{name:20s}: {value:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate F1-Score\n",
    "precision = results[3]\n",
    "recall = results[4]\n",
    "f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "print(f\"\\nF1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Example Predictions üîÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation samples\n",
    "n_examples = 6\n",
    "random_indices = np.random.choice(len(X_t0_val), n_examples, replace=False)\n",
    "\n",
    "predictions = model.predict([X_t0_val[random_indices], X_t1_val[random_indices]], verbose=0)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(n_examples, 4, figsize=(16, 4*n_examples))\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # T0\n",
    "    axes[i, 0].imshow(X_t0_val[idx])\n",
    "    axes[i, 0].set_title('T‚ÇÄ (Before)', fontsize=12, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # T1\n",
    "    axes[i, 1].imshow(X_t1_val[idx])\n",
    "    axes[i, 1].set_title('T‚ÇÅ (After)', fontsize=12, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Ground Truth\n",
    "    axes[i, 2].imshow(y_val[idx, :, :, 0], cmap='Reds', vmin=0, vmax=1)\n",
    "    axes[i, 2].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    pred = predictions[i, :, :, 0]\n",
    "    defor_pct = (pred > 0.5).sum() / pred.size * 100\n",
    "    axes[i, 3].imshow(pred, cmap='Reds', vmin=0, vmax=1)\n",
    "    axes[i, 3].set_title(f'Prediction ({defor_pct:.1f}% defor.)', fontsize=12, fontweight='bold')\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'predictions_{timestamp}.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Predictions saved: predictions_{timestamp}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Detailed Analysis üîç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze single prediction with different thresholds\n",
    "idx = 0\n",
    "test_t0 = X_t0_val[idx:idx+1]\n",
    "test_t1 = X_t1_val[idx:idx+1]\n",
    "true_mask = y_val[idx:idx+1]\n",
    "\n",
    "pred_mask = model.predict([test_t0, test_t1], verbose=0)\n",
    "\n",
    "# Different thresholds\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Top row\n",
    "axes[0, 0].imshow(test_t0[0])\n",
    "axes[0, 0].set_title('T‚ÇÄ (Before)', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(test_t1[0])\n",
    "axes[0, 1].set_title('T‚ÇÅ (After)', fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(true_mask[0, :, :, 0], cmap='Reds')\n",
    "axes[0, 2].set_title('Ground Truth', fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "im = axes[0, 3].imshow(pred_mask[0, :, :, 0], cmap='jet', vmin=0, vmax=1)\n",
    "axes[0, 3].set_title('Probability Map', fontweight='bold')\n",
    "axes[0, 3].axis('off')\n",
    "plt.colorbar(im, ax=axes[0, 3], fraction=0.046)\n",
    "\n",
    "# Bottom row - different thresholds\n",
    "for i, thresh in enumerate(thresholds):\n",
    "    binary = (pred_mask[0, :, :, 0] > thresh).astype(float)\n",
    "    defor_pct = binary.sum() / binary.size * 100\n",
    "    axes[1, i].imshow(binary, cmap='Reds')\n",
    "    axes[1, i].set_title(f'Threshold={thresh}\\n({defor_pct:.1f}% defor.)', fontweight='bold')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# Error map\n",
    "diff = np.abs(pred_mask[0, :, :, 0] - true_mask[0, :, :, 0])\n",
    "im = axes[1, 3].imshow(diff, cmap='RdYlGn_r', vmin=0, vmax=1)\n",
    "axes[1, 3].set_title('Absolute Error', fontweight='bold')\n",
    "axes[1, 3].axis('off')\n",
    "plt.colorbar(im, ax=axes[1, 3], fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nüìä DETAILED STATISTICS\\n\")\n",
    "print(f\"Mean Absolute Error: {diff.mean():.4f}\")\n",
    "print(f\"Max Error: {diff.max():.4f}\")\n",
    "print(f\"Pixels correctly classified (threshold=0.5): {((pred_mask[0,:,:,0] > 0.5) == (true_mask[0,:,:,0] > 0.5)).sum() / true_mask.size * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model üíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = f'change_detection_final_{timestamp}.keras'\n",
    "model.save(final_model_path)\n",
    "print(f\"‚úì Model saved: {final_model_path}\")\n",
    "\n",
    "print(\"\\nüì¶ MODEL READY FOR DEPLOYMENT!\")\n",
    "print(\"\\nTo load:\")\n",
    "print(f\">>> model = keras.models.load_model('{final_model_path}')\")\n",
    "print(\"\\nTo predict:\")\n",
    "print(\">>> prediction = model.predict([img_t0, img_t1])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Production Helper Function üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_deforestation(model_path, img_t0_path, img_t1_path, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Complete function for predicting deforestation from image files\n",
    "    \n",
    "    Args:\n",
    "        model_path: path to .keras model\n",
    "        img_t0_path: path to T0 image\n",
    "        img_t1_path: path to T1 image\n",
    "        threshold: binarization threshold\n",
    "    \n",
    "    Returns:\n",
    "        dict with results and statistics\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    \n",
    "    # Load model\n",
    "    model = keras.models.load_model(model_path, compile=False)\n",
    "    \n",
    "    # Load and preprocess images\n",
    "    img_t0 = cv2.imread(img_t0_path)\n",
    "    img_t1 = cv2.imread(img_t1_path)\n",
    "    \n",
    "    img_t0 = cv2.cvtColor(img_t0, cv2.COLOR_BGR2RGB)\n",
    "    img_t1 = cv2.cvtColor(img_t1, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img_t0 = cv2.resize(img_t0, (256, 256)) / 255.0\n",
    "    img_t1 = cv2.resize(img_t1, (256, 256)) / 255.0\n",
    "    \n",
    "    img_t0 = np.expand_dims(img_t0, axis=0)\n",
    "    img_t1 = np.expand_dims(img_t1, axis=0)\n",
    "    \n",
    "    # Predict\n",
    "    prob_mask = model.predict([img_t0, img_t1], verbose=0)\n",
    "    binary_mask = (prob_mask > threshold).astype(np.uint8)\n",
    "    \n",
    "    # Statistics\n",
    "    deforestation_pixels = binary_mask.sum()\n",
    "    total_pixels = binary_mask.size\n",
    "    deforestation_percentage = (deforestation_pixels / total_pixels) * 100\n",
    "    \n",
    "    results = {\n",
    "        'probability_map': prob_mask[0, :, :, 0],\n",
    "        'binary_mask': binary_mask[0, :, :, 0],\n",
    "        'deforestation_percentage': deforestation_percentage,\n",
    "        'deforestation_pixels': deforestation_pixels,\n",
    "        'total_pixels': total_pixels,\n",
    "        'threshold': threshold\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì Helper function defined\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\">>> results = predict_deforestation('model.keras', 'before.jpg', 'after.jpg')\")\n",
    "print(\">>> print(f'Deforestation: {results['deforestation_percentage']:.2f}%')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the Change Detection CNN tutorial!\n",
    "\n",
    "### What You've Learned:\n",
    "- ‚úÖ CNN architecture for change detection\n",
    "- ‚úÖ Combined loss functions (BCE + Dice)\n",
    "- ‚úÖ Training with callbacks\n",
    "- ‚úÖ Multiple evaluation metrics\n",
    "- ‚úÖ Threshold analysis\n",
    "- ‚úÖ Model deployment\n",
    "\n",
    "### Next Steps:\n",
    "1. **Use Real Data**: Replace synthetic data with actual drone/satellite images\n",
    "2. **Try Advanced Architecture**: Implement U-Net or Siamese CNN\n",
    "3. **Improve Performance**: Add more augmentation, try transfer learning\n",
    "4. **Deploy**: Create a web service or integrate with GIS tools\n",
    "5. **Scale Up**: Process large areas with sliding windows\n",
    "\n",
    "### Resources:\n",
    "- [TensorFlow Documentation](https://www.tensorflow.org/)\n",
    "- [Keras Examples](https://keras.io/examples/)\n",
    "- [Change Detection Papers](https://paperswithcode.com/task/change-detection)\n",
    "\n",
    "**Happy monitoring! üå≤üõ∞Ô∏è**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
